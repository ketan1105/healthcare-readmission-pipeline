# ğŸ¥ Healthcare Readmission Risk Pipeline

An industry-grade data engineering project using **PySpark** and **Google Cloud Platform (GCP)** to process real-world healthcare data and simulate readmission risk analytics. This project focuses on scalable **ETL with PySpark**, **Cloud Storage**, and **BigQuery**.

---

## ğŸ“Œ Project Objective

To build a reliable data pipeline that processes electronic health record (EHR) data from hospitals to predict and analyze patient **readmission risk** based on demographic and clinical features.

---

## ğŸ§° Tech Stack

| Layer            | Tool / Service                     |
|------------------|-------------------------------------|
| Language         | Python (PySpark)                    |
| Cloud Platform   | Google Cloud Platform               |
| Storage          | Cloud Storage (GCS)                 |
| Data Processing  | PySpark (on Dataproc or locally)    |
| Data Warehouse   | BigQuery                            |
| Dashboard        | Looker Studio (or BigQuery SQLs)    |

---

## ğŸ“‚ Project Structure

```
healthcare-readmission-pipeline/
â”œâ”€â”€ data/ # Sample or test data (optional)
â”œâ”€â”€ notebooks/ # Exploratory analysis (if any)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingestion/ # PySpark scripts to read GCS data
â”‚ â”œâ”€â”€ transformation/ # PySpark scripts for data cleaning
â”‚ â””â”€â”€ prediction/ # Logic for risk prediction
â”œâ”€â”€ configs/ # Schema files, GCP paths
â”œâ”€â”€ docs/ # Architecture diagrams, references
â”œâ”€â”€ bigquery/ # SQL queries, table DDLs
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file

```


---

## ğŸ“Š Dataset

**Source:** [UCI Diabetes Readmission Dataset](https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008)  
- 100,000+ patient records  
- Includes demographics, diagnoses, medications, readmission info  
- Format: CSV

Uploaded to:

gs://<your-bucket-name>/raw_data/diabetes_readmission.csv


---

## ğŸ”„ Data Flow Overview

1. **Raw Data Ingestion**  
   â†’ From Cloud Storage to PySpark  

2. **Data Cleaning & Transformation**  
   â†’ Null handling, feature engineering, value standardization  

3. **Risk Logic Inference**  
   â†’ Rule-based logic (e.g. age + number of inpatient visits)  

4. **Data Sink**  
   â†’ Write final output to **BigQuery**  

5. **Analysis & Reporting**  
   â†’ Run SQL queries or build Looker dashboards

---

## ğŸš§ Work in Progress

| Phase | Description                             | Status     |
|-------|-----------------------------------------|------------|
| 1     | Setup GitHub repo and choose dataset    | âœ… Done     |
| 2     | Upload data to GCS                      | â³ In Progress |
| 3     | Write PySpark ingestion job             | â³ Next     |
| 4     | Data cleaning and transformation        |            |
| 5     | Add prediction logic                    |            |
| 6     | Write to BigQuery                       |            |
| 7     | Dashboarding with Looker Studio         |            |

---

## ğŸ“Œ Author

**Ketan Jain**  
Data Engineer
[GitHub](https://github.com/ketan1105) â€¢ [LinkedIn](https://www.linkedin.com/in/ketan-jain-/)

---

## ğŸ“„ License

This project is licensed under the MIT License.
